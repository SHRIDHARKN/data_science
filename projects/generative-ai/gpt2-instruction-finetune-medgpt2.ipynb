{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n)\nfrom datasets import load_dataset\n\n# Configuration\nMODEL_NAME = \"gpt2\"\nDATASET_NAME = \"bio-nlp-umass/bioinstruct\"\nOUTPUT_DIR = \"./gpt2-bioinstruct-finetuned\"\nLEARNING_RATE = 2e-4\nBATCH_SIZE = 12\nGRADIENT_ACCUMULATION_STEPS = 12\nEPOCHS = 1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T13:39:56.852961Z","iopub.execute_input":"2025-02-22T13:39:56.853312Z","iopub.status.idle":"2025-02-22T13:40:19.318273Z","shell.execute_reply.started":"2025-02-22T13:39:56.853275Z","shell.execute_reply":"2025-02-22T13:40:19.317379Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T13:40:19.319354Z","iopub.execute_input":"2025-02-22T13:40:19.319939Z","execution_failed":"2025-02-22T14:41:57.320Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001cd8a05c0c468b85ea4a8b70df8878"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70ac874550e849508c8dc83c306b4a99"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Load dataset\ndataset = load_dataset(DATASET_NAME, split=\"train\") # using train split.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T13:40:29.665942Z","iopub.execute_input":"2025-02-22T13:40:29.666226Z","iopub.status.idle":"2025-02-22T13:40:32.295750Z","shell.execute_reply.started":"2025-02-22T13:40:29.666204Z","shell.execute_reply":"2025-02-22T13:40:32.294902Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"135405c232024bc494c8ceeecdf3903b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"biomed_instruct_25k.json:   0%|          | 0.00/13.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cc5daa3233d41a08dc43afe59a7377c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25005 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbbfe7e4922e4d02a830a19065d1f00f"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Preprocessing function (format for instruction following)\ndef preprocess_function(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n\n    prompts = [f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput: {output_text}\"\n               for instruction, input_text, output_text in zip(instructions, inputs, outputs)]\n\n    model_inputs = tokenizer(prompts, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T13:40:50.840465Z","iopub.execute_input":"2025-02-22T13:40:50.840779Z","iopub.status.idle":"2025-02-22T13:41:01.096503Z","shell.execute_reply.started":"2025-02-22T13:40:50.840752Z","shell.execute_reply":"2025-02-22T13:41:01.095315Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25005 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b33af78f3ec34b07a156a8070cdfc078"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    optim=\"adamw_torch\",\n    fp16=True,\n    lr_scheduler_type=\"linear\",\n    warmup_ratio=0.03,\n    save_strategy=\"steps\",\n    push_to_hub=False,\n    report_to = \"tensorboard\",\n    save_total_limit = 1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T13:43:29.339401Z","iopub.execute_input":"2025-02-22T13:43:29.339725Z","iopub.status.idle":"2025-02-22T13:43:29.367566Z","shell.execute_reply.started":"2025-02-22T13:43:29.339697Z","shell.execute_reply":"2025-02-22T13:43:29.366880Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T13:43:36.710886Z","iopub.execute_input":"2025-02-22T13:43:36.711208Z","iopub.status.idle":"2025-02-22T13:43:36.723949Z","shell.execute_reply.started":"2025-02-22T13:43:36.711181Z","shell.execute_reply":"2025-02-22T13:43:36.723059Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-9-c79047602355>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Train\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T13:43:40.846966Z","iopub.execute_input":"2025-02-22T13:43:40.847307Z","iopub.status.idle":"2025-02-22T14:16:05.098002Z","shell.execute_reply.started":"2025-02-22T13:43:40.847277Z","shell.execute_reply":"2025-02-22T14:16:05.097297Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='173' max='173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [173/173 32:12, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>8.529700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.194600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.464100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.423600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.394500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.396900</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.372100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.376900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.374700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.380500</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.366800</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.370600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.369300</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.365800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.351200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.354600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.358800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=173, training_loss=0.8990579464532047, metrics={'train_runtime': 1943.8994, 'train_samples_per_second': 12.863, 'train_steps_per_second': 0.089, 'total_flos': 6509307101184000.0, 'train_loss': 0.8990579464532047, 'epoch': 0.9961612284069098})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Example input (instruction and input text)\ninstruction = \"Explain the mechanism of action of a given drug in non-medical terms.\"\ninput_text = \"Metformin\"\n\n# Construct the prompt\nprompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput:\"\n\n# Tokenize the prompt\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n# UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only \n# used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n#  UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used \n# in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n\n# Generate the output\nmodel.eval()  # Set model to evaluation mode\nwith torch.no_grad():\n    outputs = model.generate(\n        **model_inputs,\n        max_length=200,  # Adjust as needed\n        num_return_sequences=1,  # Generate one output\n        temperature=0.7,  # Adjust for creativity\n        top_p=0.9,  # Adjust for sampling\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id #Set pad token id\n    )\n\n# Decode the generated output\ngenerated_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the results\nprint(\"Instruction:\", instruction)\nprint(\"Input:\", input_text)\nprint(\"Generated Output:\", generated_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:26:34.520206Z","iopub.execute_input":"2025-02-22T14:26:34.520562Z","iopub.status.idle":"2025-02-22T14:26:35.159134Z","shell.execute_reply.started":"2025-02-22T14:26:34.520536Z","shell.execute_reply":"2025-02-22T14:26:35.158442Z"}},"outputs":[{"name":"stdout","text":"Instruction: Explain the mechanism of action of a given drug in non-medical terms.\nInput: Metformin\nGenerated Output: Instruction: Explain the mechanism of action of a given drug in non-medical terms.\nInput: Metformin\nOutput: Metformin works by inhibiting the synthesis of beta-blockers, which are the chemicals involved in the production of prostaglandins, the chemicals responsible for the immune system's response to an injury.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Example input (instruction and input text)\ninstruction = \"Explain the concept of 'gene expression' in simple terms.\"\ninput_text = \"\"  # No input text is needed for this example\n\n# Construct the prompt\nprompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput:\"\n\n# Tokenize the prompt\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n# UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only \n# used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n#  UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used \n# in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n\n# Generate the output\nmodel.eval()  # Set model to evaluation mode\nwith torch.no_grad():\n    outputs = model.generate(\n        **model_inputs,\n        max_length=200,  # Adjust as needed\n        num_return_sequences=1,  # Generate one output\n        temperature=0.7,  # Adjust for creativity\n        top_p=0.9,  # Adjust for sampling\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id #Set pad token id\n    )\n\n# Decode the generated output\ngenerated_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the results\nprint(\"Instruction:\", instruction)\nprint(\"Input:\", input_text)\nprint(\"Generated Output:\", generated_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:28:14.012760Z","iopub.execute_input":"2025-02-22T14:28:14.013061Z","iopub.status.idle":"2025-02-22T14:28:14.488391Z","shell.execute_reply.started":"2025-02-22T14:28:14.013037Z","shell.execute_reply":"2025-02-22T14:28:14.487679Z"}},"outputs":[{"name":"stdout","text":"Instruction: Explain the concept of 'gene expression' in simple terms.\nInput: \nGenerated Output: Instruction: Explain the concept of 'gene expression' in simple terms.\nInput: \nOutput: Gene expression refers to the process of producing new proteins or nucleic acids. It is a crucial part of the immune system and plays a critical role in maintaining healthy immune function and overall health.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Example input (instruction and input text)\ninstruction = \"Answer the following question.\"\ninput_text = \"What are the main causes of type 2 diabetes?\"\n\n# Construct the prompt\nprompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput:\"\n\n# Tokenize the prompt\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n# UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only \n# used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n#  UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used \n# in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n\n# Generate the output\nmodel.eval()  # Set model to evaluation mode\nwith torch.no_grad():\n    outputs = model.generate(\n        **model_inputs,\n        max_length=200,  # Adjust as needed\n        num_return_sequences=1,  # Generate one output\n        temperature=0.7,  # Adjust for creativity\n        top_p=0.9,  # Adjust for sampling\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id #Set pad token id\n    )\n\n# Decode the generated output\ngenerated_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the results\nprint(\"Instruction:\", instruction)\nprint(\"Input:\", input_text)\nprint(\"Generated Output:\", generated_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:36:13.024674Z","iopub.execute_input":"2025-02-22T14:36:13.024999Z","iopub.status.idle":"2025-02-22T14:36:13.238955Z","shell.execute_reply.started":"2025-02-22T14:36:13.024974Z","shell.execute_reply":"2025-02-22T14:36:13.238123Z"}},"outputs":[{"name":"stdout","text":"Instruction: Answer the following question.\nInput: What are the main causes of type 2 diabetes?\nGenerated Output: Instruction: Answer the following question.\nInput: What are the main causes of type 2 diabetes?\nOutput: Main causes of type 2 diabetes include obesity, high blood pressure, and smoking.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!zip -r \"gpt2_bio_instruct_ft.zip\" \"/kaggle/working\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:38:19.059520Z","iopub.execute_input":"2025-02-22T14:38:19.059868Z","iopub.status.idle":"2025-02-22T14:39:32.452354Z","shell.execute_reply.started":"2025-02-22T14:38:19.059837Z","shell.execute_reply":"2025-02-22T14:39:32.451157Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/logs/ (stored 0%)\n  adding: kaggle/working/logs/events.out.tfevents.1740231821.cfdb00881f52.31.0 (deflated 62%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/ (stored 0%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/ (stored 0%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/generation_config.json (deflated 24%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/merges.txt (deflated 53%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/rng_state.pth (deflated 25%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/config.json (deflated 51%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/tokenizer_config.json (deflated 54%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/training_args.bin (deflated 52%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/optimizer.pt (deflated 8%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/vocab.json (deflated 59%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/scheduler.pt (deflated 56%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/trainer_state.json (deflated 71%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/model.safetensors (deflated 7%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/special_tokens_map.json (deflated 60%)\n  adding: kaggle/working/gpt2-bioinstruct-finetuned/checkpoint-173/tokenizer.json (deflated 82%)\n","output_type":"stream"}],"execution_count":17}]}