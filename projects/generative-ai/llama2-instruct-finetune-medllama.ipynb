{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U accelerate peft bitsandbytes\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n)\nfrom datasets import load_dataset\nfrom peft import LoraConfig,get_peft_model  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:09:19.405469Z","iopub.execute_input":"2025-02-23T06:09:19.405956Z","iopub.status.idle":"2025-02-23T06:09:30.831890Z","shell.execute_reply.started":"2025-02-23T06:09:19.405916Z","shell.execute_reply":"2025-02-23T06:09:30.830861Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\ntoken=\"\"\nlogin(token=token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:09:30.832969Z","iopub.execute_input":"2025-02-23T06:09:30.833508Z","iopub.status.idle":"2025-02-23T06:09:30.956301Z","shell.execute_reply.started":"2025-02-23T06:09:30.833468Z","shell.execute_reply":"2025-02-23T06:09:30.955419Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Configuration\n#MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"  # Or your Llama 2 model\nMODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # Or your Llama 2 model\nDATASET_NAME = \"bio-nlp-umass/bioinstruct\"\nOUTPUT_DIR = \"./llama2-bioinstruct-finetuned\"\nLEARNING_RATE = 2e-4\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION_STEPS = 8\nEPOCHS = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:09:30.957881Z","iopub.execute_input":"2025-02-23T06:09:30.958144Z","iopub.status.idle":"2025-02-23T06:09:30.962119Z","shell.execute_reply.started":"2025-02-23T06:09:30.958121Z","shell.execute_reply":"2025-02-23T06:09:30.961234Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Quantization configuration\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# quantization_config = BitsAndBytesConfig(\n#     load_in_8bit=True, # Load in 8 bit\n#     bnb_8bit_quant_type=\"nf8\", # specify nf8 type\n#     bnb_8bit_compute_dtype=torch.float16, # compute type\n# )\n\n# quantization_config = BitsAndBytesConfig(\n#     load_in_8bit=True,\n# )\n\n# Load model and tokenizer with quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n)\n\n# LoRA configuration\nlora_config = LoraConfig(\n    r=8,  # Rank of the update matrices\n    lora_alpha=8,  # Scaling factor\n    lora_dropout=0.1,  # Dropout probability\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\", # Set the correct task type\n    target_modules=[\"v_proj\"] # Target Query and Value projection matrices.\n)\n\n# lora_config = LoraConfig(\n#     r=8,\n#     lora_alpha=8,\n#     lora_dropout=0.1,\n#     bias=\"none\",\n#     task_type=\"CAUSAL_LM\",\n#     target_modules=[\"q_proj\", \"v_proj\"]\n# )\n\n# Add LoRA adapters to the model\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters() # print trainable parameters.\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:09:30.963844Z","iopub.execute_input":"2025-02-23T06:09:30.964074Z","iopub.status.idle":"2025-02-23T06:09:37.848365Z","shell.execute_reply.started":"2025-02-23T06:09:30.964054Z","shell.execute_reply":"2025-02-23T06:09:37.847349Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f75480956a9c43f0a0a73d795b5e74fe"}},"metadata":{}},{"name":"stdout","text":"trainable params: 2,097,152 || all params: 6,740,512,768 || trainable%: 0.0311\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Load dataset\ndataset = load_dataset(DATASET_NAME, split=\"train\")\ndataset = dataset.select(range(10000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:09:37.849334Z","iopub.execute_input":"2025-02-23T06:09:37.849580Z","iopub.status.idle":"2025-02-23T06:09:38.683765Z","shell.execute_reply.started":"2025-02-23T06:09:37.849560Z","shell.execute_reply":"2025-02-23T06:09:38.682723Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Preprocessing function (format for instruction following)\ndef preprocess_function(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n\n    prompts = [f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput: {output_text}\"\n               for instruction, input_text, output_text in zip(instructions, inputs, outputs)]\n\n    model_inputs = tokenizer(prompts, max_length=512, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:09:38.684863Z","iopub.execute_input":"2025-02-23T06:09:38.685187Z","iopub.status.idle":"2025-02-23T06:09:42.626420Z","shell.execute_reply.started":"2025-02-23T06:09:38.685158Z","shell.execute_reply":"2025-02-23T06:09:42.625726Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46f985abdd9642a38fdb8df20467ec29"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    optim=\"paged_adamw_8bit\", # Optimized for memory\n    fp16=True,\n    lr_scheduler_type=\"linear\",\n    warmup_ratio=0.03,\n    save_strategy=\"steps\",\n    push_to_hub=False,\n    report_to = \"tensorboard\",\n    save_total_limit = 1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:09:42.627142Z","iopub.execute_input":"2025-02-23T06:09:42.627347Z","iopub.status.idle":"2025-02-23T06:09:42.660548Z","shell.execute_reply.started":"2025-02-23T06:09:42.627329Z","shell.execute_reply":"2025-02-23T06:09:42.659921Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:09:42.661278Z","iopub.execute_input":"2025-02-23T06:09:42.661502Z","iopub.status.idle":"2025-02-23T06:09:42.899352Z","shell.execute_reply.started":"2025-02-23T06:09:42.661478Z","shell.execute_reply":"2025-02-23T06:09:42.898371Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-8-1d3c410372ae>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Train\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T06:09:42.901221Z","iopub.execute_input":"2025-02-23T06:09:42.901491Z","iopub.status.idle":"2025-02-23T08:30:25.422127Z","shell.execute_reply.started":"2025-02-23T06:09:42.901466Z","shell.execute_reply":"2025-02-23T08:30:25.421302Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [312/312 2:20:14, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>81.500200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.785900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.662300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.651500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.650800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.622100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=312, training_loss=14.46341899725107, metrics={'train_runtime': 8442.1885, 'train_samples_per_second': 1.185, 'train_steps_per_second': 0.037, 'total_flos': 2.0271715316033126e+17, 'train_loss': 14.46341899725107, 'epoch': 0.9984})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Example input (instruction and input text)\ninstruction = \"Explain the mechanism of action of a given drug in non-medical terms.\"\ninput_text = \"Metformin\"\n\n# Construct the prompt\nprompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput:\"\n\n# Tokenize the prompt\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n# UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only \n# used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n#  UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used \n# in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n\n# Generate the output\nmodel.eval()  # Set model to evaluation mode\nwith torch.no_grad():\n    outputs = model.generate(\n        **model_inputs,\n        max_length=200,  # Adjust as needed\n        num_return_sequences=1,  # Generate one output\n        temperature=0.7,  # Adjust for creativity\n        top_p=0.9,  # Adjust for sampling\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id #Set pad token id\n    )\n\n# Decode the generated output\ngenerated_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the results\nprint(\"Instruction:\", instruction)\nprint(\"Input:\", input_text)\nprint(\"Generated Output:\", generated_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T08:34:35.179829Z","iopub.execute_input":"2025-02-23T08:34:35.180215Z","iopub.status.idle":"2025-02-23T08:34:40.227911Z","shell.execute_reply.started":"2025-02-23T08:34:35.180188Z","shell.execute_reply":"2025-02-23T08:34:40.226770Z"}},"outputs":[{"name":"stdout","text":"Instruction: Explain the mechanism of action of a given drug in non-medical terms.\nInput: Metformin\nGenerated Output: Instruction: Explain the mechanism of action of a given drug in non-medical terms.\nInput: Metformin\nOutput: Metformin helps reduce blood sugar levels by increasing the sensitivity of cells in the body to insulin, allowing the insulin to work more effectively. It also decreases the amount of glucose produced by the liver and increases the body's use of glucose.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Example input (instruction and input text)\ninstruction = \"Explain the concept of 'gene expression' in simple terms.\"\ninput_text = \"\"  # No input text is needed for this example\n\n# Construct the prompt\nprompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput:\"\n\n# Tokenize the prompt\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n# UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only \n# used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n#  UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used \n# in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n\n# Generate the output\nmodel.eval()  # Set model to evaluation mode\nwith torch.no_grad():\n    outputs = model.generate(\n        **model_inputs,\n        max_length=200,  # Adjust as needed\n        num_return_sequences=1,  # Generate one output\n        temperature=0.7,  # Adjust for creativity\n        top_p=0.9,  # Adjust for sampling\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id #Set pad token id\n    )\n\n# Decode the generated output\ngenerated_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the results\nprint(\"Instruction:\", instruction)\nprint(\"Input:\", input_text)\nprint(\"Generated Output:\", generated_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T08:34:55.475406Z","iopub.execute_input":"2025-02-23T08:34:55.475781Z","iopub.status.idle":"2025-02-23T08:34:59.990751Z","shell.execute_reply.started":"2025-02-23T08:34:55.475747Z","shell.execute_reply":"2025-02-23T08:34:59.989862Z"}},"outputs":[{"name":"stdout","text":"Instruction: Explain the concept of 'gene expression' in simple terms.\nInput: \nGenerated Output: Instruction: Explain the concept of 'gene expression' in simple terms.\nInput: \nOutput: Gene expression refers to the process of how genes are turned into functional proteins in the body. It's the result of various factors, including DNA methylation, chromatin remodeling, and RNA splicing. Gene expression can be influenced by external factors like environmental exposure and lifestyle choices.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Example input (instruction and input text)\ninstruction = \"Answer the following question.\"\ninput_text = \"What are the main causes of type 2 diabetes?\"\n\n# Construct the prompt\nprompt = f\"Instruction: {instruction}\\nInput: {input_text}\\nOutput:\"\n\n# Tokenize the prompt\nmodel_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n# UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only \n# used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n#  UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used \n# in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n\n# Generate the output\nmodel.eval()  # Set model to evaluation mode\nwith torch.no_grad():\n    outputs = model.generate(\n        **model_inputs,\n        max_length=200,  # Adjust as needed\n        num_return_sequences=1,  # Generate one output\n        temperature=0.7,  # Adjust for creativity\n        top_p=0.9,  # Adjust for sampling\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id #Set pad token id\n    )\n\n# Decode the generated output\ngenerated_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the results\nprint(\"Instruction:\", instruction)\nprint(\"Input:\", input_text)\nprint(\"Generated Output:\", generated_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T08:35:16.920336Z","iopub.execute_input":"2025-02-23T08:35:16.920650Z","iopub.status.idle":"2025-02-23T08:35:20.584246Z","shell.execute_reply.started":"2025-02-23T08:35:16.920626Z","shell.execute_reply":"2025-02-23T08:35:20.583301Z"}},"outputs":[{"name":"stdout","text":"Instruction: Answer the following question.\nInput: What are the main causes of type 2 diabetes?\nGenerated Output: Instruction: Answer the following question.\nInput: What are the main causes of type 2 diabetes?\nOutput: The main causes of type 2 diabetes include a combination of genetic, environmental, and lifestyle factors. These include age, obesity, a sedentary lifestyle, family history, ethnicity, and poor diet.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!zip -r \"llama2_bio_instruct_ft.zip\" \"/kaggle/working\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T08:36:10.262196Z","iopub.execute_input":"2025-02-23T08:36:10.262495Z","iopub.status.idle":"2025-02-23T08:36:11.314338Z","shell.execute_reply.started":"2025-02-23T08:36:10.262473Z","shell.execute_reply":"2025-02-23T08:36:11.313461Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/ (stored 0%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/ (stored 0%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/optimizer.pt (deflated 11%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/special_tokens_map.json (deflated 73%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/tokenizer_config.json (deflated 68%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/training_args.bin (deflated 51%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/trainer_state.json (deflated 64%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/adapter_config.json (deflated 54%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/tokenizer.json (deflated 85%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/scheduler.pt (deflated 55%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/rng_state.pth (deflated 25%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/tokenizer.model (deflated 55%)\n  adding: kaggle/working/llama2-bioinstruct-finetuned/checkpoint-312/README.md (deflated 66%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/logs/ (stored 0%)\n  adding: kaggle/working/logs/events.out.tfevents.1740290688.300ed2617f42.109.0 (deflated 62%)\n  adding: kaggle/working/logs/events.out.tfevents.1740290883.300ed2617f42.211.0 (deflated 62%)\n  adding: kaggle/working/logs/events.out.tfevents.1740290983.300ed2617f42.263.0 (deflated 60%)\n  adding: kaggle/working/logs/events.out.tfevents.1740290842.300ed2617f42.161.0 (deflated 62%)\n  adding: kaggle/working/logs/events.out.tfevents.1740290596.300ed2617f42.31.0 (deflated 63%)\n","output_type":"stream"}],"execution_count":13}]}