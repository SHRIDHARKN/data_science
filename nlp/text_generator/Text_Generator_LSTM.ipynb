{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "NMpNFJyvAm00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Isrt9HanyFGm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.core import Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from kerastuner.tuners import BayesianOptimization\n",
        "import pickle\n",
        "import heapq\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/DollsHouse.txt'\n",
        "text = open(path).read().lower()\n",
        "print('corpus length:', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoEQ7ww6yd1U",
        "outputId": "99a80ef8-2f4f-4f51-d32d-3010fb9215da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus length: 161433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "words = tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "1rBTkgKhyd62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "  \n",
        "def remove_non_vocab_words(words_ls):\n",
        "    #sentence = nltk.wordpunct_tokenize(sentence)\n",
        "    res = [x for x in words_ls if x in vocab_words]\n",
        "    return res\n",
        "\n",
        "vocab_words = set(nltk.corpus.words.words())\n",
        "words = remove_non_vocab_words(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Djo10uxxyd8s",
        "outputId": "0e2bcbd1-3bbe-4c43-b8ac-52afd4efa348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words = np.unique(words).tolist()\n",
        "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))"
      ],
      "metadata": {
        "id": "eokLMvPJyd-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_LENGTH = 5\n",
        "prev_words = []\n",
        "next_words = []\n",
        "for i in range(len(words) - WORD_LENGTH):\n",
        "    prev_words.append(words[i:i + WORD_LENGTH])\n",
        "    next_words.append(words[i + WORD_LENGTH])\n",
        "print(prev_words[0])\n",
        "print(next_words[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JddtY0VEytFG",
        "outputId": "79f36a60-47ac-45be-fb5d-821231c0e0d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'project', 'of', 'a', 'doll']\n",
            "s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(prev_words), WORD_LENGTH, len(unique_words)), dtype=bool)\n",
        "Y = np.zeros((len(next_words), len(unique_words)), dtype=bool)\n",
        "\n",
        "for i, each_words in enumerate(prev_words):\n",
        "    for j, each_word in enumerate(each_words):\n",
        "        X[i, j, unique_word_index[each_word]] = 1\n",
        "    Y[i, unique_word_index[next_words[i]]] = 1"
      ],
      "metadata": {
        "id": "OYRjxv0CytHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kerastuner.tuners import RandomSearch"
      ],
      "metadata": {
        "id": "_fH6XFenE7si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=hp.Int('units',min_value=50,max_value=250,step=25),\\\n",
        "                   activation='relu', input_shape=(WORD_LENGTH, len(unique_words))))\n",
        "    \n",
        "    model.add(Dense(len(unique_words)))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(\\\n",
        "            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                   metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "jGN2I-sSDPW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='accuracy',\n",
        "    max_trials=5,\n",
        "    directory='/content/res/',\n",
        "    project_name='Text Gen')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt0LuouuFJzB",
        "outputId": "9cb2ebb0-207e-425b-da74-0d3d455378ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2TtUPTGFWKv",
        "outputId": "cd64b872-1009-4eb5-ac5d-f51016b26b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 2\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 50, 'max_value': 250, 'step': 25, 'sampling': None}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(X, Y,epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5wlLdC8FaHH",
        "outputId": "0bb8968b-c5f1-44a1-82da-8448454cb4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 02m 23s]\n",
            "accuracy: 0.25599607825279236\n",
            "\n",
            "Best accuracy So Far: 0.6981061100959778\n",
            "Total elapsed time: 00h 10m 40s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5W1j3MTFqEr",
        "outputId": "579eb10f-8322-49c0-94e5-1a7cb677dc3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in /content/res/Text Gen\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7fb0ac471550>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 125\n",
            "learning_rate: 0.01\n",
            "Score: 0.6981061100959778\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 175\n",
            "learning_rate: 0.001\n",
            "Score: 0.25599607825279236\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 100\n",
            "learning_rate: 0.001\n",
            "Score: 0.18344064056873322\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 125\n",
            "learning_rate: 0.0001\n",
            "Score: 0.04680145904421806\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 75\n",
            "learning_rate: 0.0001\n",
            "Score: 0.04156782850623131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dropout"
      ],
      "metadata": {
        "id": "8BbfXOG4SWHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(125, input_shape=(WORD_LENGTH, len(unique_words))))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(unique_words)))\n",
        "model.add(Activation('softmax'))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "#history = model.fit(X, Y, validation_split=0.1, batch_size=256, epochs=40, shuffle=True).history\n",
        "history = model.fit(X, Y, batch_size=128, epochs=150, shuffle=True).history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsghTKkoytJv",
        "outputId": "f089dfce-70b7-4acb-c5c9-161973cdc70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "208/208 [==============================] - 3s 7ms/step - loss: 6.1640 - accuracy: 0.0384\n",
            "Epoch 2/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 5.7398 - accuracy: 0.0401\n",
            "Epoch 3/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 5.6984 - accuracy: 0.0439\n",
            "Epoch 4/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 5.6475 - accuracy: 0.0527\n",
            "Epoch 5/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 5.5643 - accuracy: 0.0590\n",
            "Epoch 6/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 5.4587 - accuracy: 0.0755\n",
            "Epoch 7/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 5.3513 - accuracy: 0.0847\n",
            "Epoch 8/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 5.2452 - accuracy: 0.0974\n",
            "Epoch 9/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 5.1462 - accuracy: 0.1056\n",
            "Epoch 10/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 5.0520 - accuracy: 0.1166\n",
            "Epoch 11/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 4.9611 - accuracy: 0.1250\n",
            "Epoch 12/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 4.8651 - accuracy: 0.1339\n",
            "Epoch 13/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 4.7757 - accuracy: 0.1435\n",
            "Epoch 14/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 4.6845 - accuracy: 0.1501\n",
            "Epoch 15/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 4.5902 - accuracy: 0.1604\n",
            "Epoch 16/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 4.5064 - accuracy: 0.1662\n",
            "Epoch 17/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 4.4139 - accuracy: 0.1747\n",
            "Epoch 18/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 4.3282 - accuracy: 0.1842\n",
            "Epoch 19/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 4.2318 - accuracy: 0.1894\n",
            "Epoch 20/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 4.1509 - accuracy: 0.1999\n",
            "Epoch 21/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 4.0627 - accuracy: 0.2107\n",
            "Epoch 22/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 3.9768 - accuracy: 0.2200\n",
            "Epoch 23/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 3.8983 - accuracy: 0.2290\n",
            "Epoch 24/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 3.8098 - accuracy: 0.2397\n",
            "Epoch 25/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 3.7295 - accuracy: 0.2501\n",
            "Epoch 26/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 3.6497 - accuracy: 0.2575\n",
            "Epoch 27/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 3.5677 - accuracy: 0.2699\n",
            "Epoch 28/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 3.4886 - accuracy: 0.2814\n",
            "Epoch 29/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 3.4061 - accuracy: 0.2921\n",
            "Epoch 30/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 3.3347 - accuracy: 0.3010\n",
            "Epoch 31/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 3.2529 - accuracy: 0.3136\n",
            "Epoch 32/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 3.1772 - accuracy: 0.3268\n",
            "Epoch 33/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 3.0955 - accuracy: 0.3409\n",
            "Epoch 34/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 3.0245 - accuracy: 0.3516\n",
            "Epoch 35/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 2.9506 - accuracy: 0.3629\n",
            "Epoch 36/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 2.8816 - accuracy: 0.3775\n",
            "Epoch 37/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 2.8045 - accuracy: 0.3905\n",
            "Epoch 38/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 2.7346 - accuracy: 0.4030\n",
            "Epoch 39/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 2.6657 - accuracy: 0.4195\n",
            "Epoch 40/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 2.6015 - accuracy: 0.4295\n",
            "Epoch 41/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 2.5365 - accuracy: 0.4401\n",
            "Epoch 42/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 2.4701 - accuracy: 0.4580\n",
            "Epoch 43/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 2.4102 - accuracy: 0.4690\n",
            "Epoch 44/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 2.3475 - accuracy: 0.4793\n",
            "Epoch 45/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 2.2894 - accuracy: 0.4934\n",
            "Epoch 46/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 2.2368 - accuracy: 0.5054\n",
            "Epoch 47/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 2.1719 - accuracy: 0.5162\n",
            "Epoch 48/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 2.1188 - accuracy: 0.5273\n",
            "Epoch 49/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 2.0627 - accuracy: 0.5402\n",
            "Epoch 50/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 2.0209 - accuracy: 0.5481\n",
            "Epoch 51/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 1.9679 - accuracy: 0.5589\n",
            "Epoch 52/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.9126 - accuracy: 0.5717\n",
            "Epoch 53/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.8646 - accuracy: 0.5827\n",
            "Epoch 54/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.8213 - accuracy: 0.5887\n",
            "Epoch 55/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.7740 - accuracy: 0.6010\n",
            "Epoch 56/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 1.7279 - accuracy: 0.6110\n",
            "Epoch 57/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.6866 - accuracy: 0.6185\n",
            "Epoch 58/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.6442 - accuracy: 0.6249\n",
            "Epoch 59/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 1.6058 - accuracy: 0.6354\n",
            "Epoch 60/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 1.5699 - accuracy: 0.6449\n",
            "Epoch 61/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.5322 - accuracy: 0.6492\n",
            "Epoch 62/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.4968 - accuracy: 0.6566\n",
            "Epoch 63/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.4506 - accuracy: 0.6663\n",
            "Epoch 64/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.4202 - accuracy: 0.6730\n",
            "Epoch 65/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.3827 - accuracy: 0.6829\n",
            "Epoch 66/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.3493 - accuracy: 0.6895\n",
            "Epoch 67/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.3253 - accuracy: 0.6929\n",
            "Epoch 68/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 1.2879 - accuracy: 0.7013\n",
            "Epoch 69/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.2573 - accuracy: 0.7074\n",
            "Epoch 70/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 1.2317 - accuracy: 0.7129\n",
            "Epoch 71/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 1.1944 - accuracy: 0.7210\n",
            "Epoch 72/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.1659 - accuracy: 0.7292\n",
            "Epoch 73/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 1.1483 - accuracy: 0.7305\n",
            "Epoch 74/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.1115 - accuracy: 0.7405\n",
            "Epoch 75/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.0899 - accuracy: 0.7445\n",
            "Epoch 76/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.0578 - accuracy: 0.7501\n",
            "Epoch 77/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.0408 - accuracy: 0.7524\n",
            "Epoch 78/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 1.0150 - accuracy: 0.7598\n",
            "Epoch 79/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.9870 - accuracy: 0.7648\n",
            "Epoch 80/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 0.9689 - accuracy: 0.7686\n",
            "Epoch 81/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 0.9461 - accuracy: 0.7721\n",
            "Epoch 82/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.9128 - accuracy: 0.7859\n",
            "Epoch 83/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 0.8908 - accuracy: 0.7884\n",
            "Epoch 84/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.8745 - accuracy: 0.7903\n",
            "Epoch 85/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.8596 - accuracy: 0.7971\n",
            "Epoch 86/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.8342 - accuracy: 0.7993\n",
            "Epoch 87/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 0.8136 - accuracy: 0.8064\n",
            "Epoch 88/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.7941 - accuracy: 0.8113\n",
            "Epoch 89/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 0.7765 - accuracy: 0.8122\n",
            "Epoch 90/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.7600 - accuracy: 0.8174\n",
            "Epoch 91/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.7455 - accuracy: 0.8208\n",
            "Epoch 92/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.7271 - accuracy: 0.8224\n",
            "Epoch 93/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.7062 - accuracy: 0.8294\n",
            "Epoch 94/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.6875 - accuracy: 0.8326\n",
            "Epoch 95/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.6769 - accuracy: 0.8361\n",
            "Epoch 96/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.6579 - accuracy: 0.8411\n",
            "Epoch 97/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.6458 - accuracy: 0.8433\n",
            "Epoch 98/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.6321 - accuracy: 0.8463\n",
            "Epoch 99/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.6147 - accuracy: 0.8503\n",
            "Epoch 100/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 0.5993 - accuracy: 0.8536\n",
            "Epoch 101/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.5857 - accuracy: 0.8558\n",
            "Epoch 102/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.5667 - accuracy: 0.8633\n",
            "Epoch 103/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.5626 - accuracy: 0.8638\n",
            "Epoch 104/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.5497 - accuracy: 0.8654\n",
            "Epoch 105/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.5307 - accuracy: 0.8685\n",
            "Epoch 106/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 0.5250 - accuracy: 0.8726\n",
            "Epoch 107/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.5132 - accuracy: 0.8748\n",
            "Epoch 108/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.5030 - accuracy: 0.8769\n",
            "Epoch 109/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.4854 - accuracy: 0.8789\n",
            "Epoch 110/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.4795 - accuracy: 0.8814\n",
            "Epoch 111/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 0.4649 - accuracy: 0.8857\n",
            "Epoch 112/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.4592 - accuracy: 0.8851\n",
            "Epoch 113/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.4436 - accuracy: 0.8908\n",
            "Epoch 114/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.4369 - accuracy: 0.8927\n",
            "Epoch 115/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.4280 - accuracy: 0.8934\n",
            "Epoch 116/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.4186 - accuracy: 0.8950\n",
            "Epoch 117/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.4005 - accuracy: 0.9029\n",
            "Epoch 118/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.4010 - accuracy: 0.9006\n",
            "Epoch 119/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.3971 - accuracy: 0.9005\n",
            "Epoch 120/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.3846 - accuracy: 0.9052\n",
            "Epoch 121/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.3783 - accuracy: 0.9055\n",
            "Epoch 122/150\n",
            "208/208 [==============================] - 2s 10ms/step - loss: 0.3636 - accuracy: 0.9104\n",
            "Epoch 123/150\n",
            "208/208 [==============================] - 2s 10ms/step - loss: 0.3590 - accuracy: 0.9109\n",
            "Epoch 124/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.3516 - accuracy: 0.9127\n",
            "Epoch 125/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.3392 - accuracy: 0.9163\n",
            "Epoch 126/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.3340 - accuracy: 0.9165\n",
            "Epoch 127/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.3290 - accuracy: 0.9185\n",
            "Epoch 128/150\n",
            "208/208 [==============================] - 1s 6ms/step - loss: 0.3242 - accuracy: 0.9184\n",
            "Epoch 129/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.3153 - accuracy: 0.9201\n",
            "Epoch 130/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.3092 - accuracy: 0.9227\n",
            "Epoch 131/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.3025 - accuracy: 0.9248\n",
            "Epoch 132/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.2982 - accuracy: 0.9246\n",
            "Epoch 133/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.2917 - accuracy: 0.9283\n",
            "Epoch 134/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.2858 - accuracy: 0.9289\n",
            "Epoch 135/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.2777 - accuracy: 0.9294\n",
            "Epoch 136/150\n",
            "208/208 [==============================] - 2s 9ms/step - loss: 0.2758 - accuracy: 0.9308\n",
            "Epoch 137/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.2667 - accuracy: 0.9345\n",
            "Epoch 138/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.2618 - accuracy: 0.9356\n",
            "Epoch 139/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.2592 - accuracy: 0.9350\n",
            "Epoch 140/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.2515 - accuracy: 0.9374\n",
            "Epoch 141/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.2519 - accuracy: 0.9372\n",
            "Epoch 142/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.2352 - accuracy: 0.9436\n",
            "Epoch 143/150\n",
            "208/208 [==============================] - 2s 12ms/step - loss: 0.2379 - accuracy: 0.9393\n",
            "Epoch 144/150\n",
            "208/208 [==============================] - 2s 9ms/step - loss: 0.2329 - accuracy: 0.9423\n",
            "Epoch 145/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.2338 - accuracy: 0.9396\n",
            "Epoch 146/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.2261 - accuracy: 0.9430\n",
            "Epoch 147/150\n",
            "208/208 [==============================] - 2s 8ms/step - loss: 0.2188 - accuracy: 0.9468\n",
            "Epoch 148/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.2132 - accuracy: 0.9489\n",
            "Epoch 149/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.2144 - accuracy: 0.9468\n",
            "Epoch 150/150\n",
            "208/208 [==============================] - 1s 7ms/step - loss: 0.2085 - accuracy: 0.9494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('keras_next_word_model.h5')\n",
        "pickle.dump(history, open(\"history.p\", \"wb\"))\n",
        "model = load_model('keras_next_word_model.h5')\n",
        "history = pickle.load(open(\"history.p\", \"rb\"))"
      ],
      "metadata": {
        "id": "pStW2n6xzK5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,4))\n",
        "plt.plot(history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "3swaugNTzfoI",
        "outputId": "858d8516-001b-4f01-8c3b-80547354af29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, top_n=3):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds)\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "    return heapq.nlargest(top_n, range(len(preds)), preds.take)"
      ],
      "metadata": {
        "id": "tv4DHCD2zhRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = \"he was grasping for\"\n",
        "SEQUENCE_LENGTH = len(test.split())\n",
        "x = np.zeros((1, WORD_LENGTH, len(unique_words)))    \n",
        "\n",
        "for t, char in enumerate(test.split()):\n",
        "        x[0, t, unique_word_index.get(char,np.random.randint(0,len(unique_words)))] = 1.   "
      ],
      "metadata": {
        "id": "FbA7iazFzkWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(x, verbose=0)[0]\n",
        "preds    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiPppQY9zqiB",
        "outputId": "562cb334-c1c3-4d2b-d329-f958f78eae04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.4914481e-01, 5.8111463e-12, 8.1479739e-09, ..., 9.3069730e-07,\n",
              "       1.2041421e-15, 4.4073943e-14], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_indices = sample(preds, 10)\n",
        "next_char = np.array(unique_words)[next_indices]\n",
        "next_char        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5txeOwqJg6N",
        "outputId": "b475aad0-bc31-4a28-f5cc-620e7cfeddb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['my', 'his', 'a', 'some', 'important', 'letter', 'money', 'in',\n",
              "       'every', 'three'], dtype='<U15')"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sent = [\"i was expecting that \",\"the one that kills us \",\"could you please step out for \"]\n",
        "\n",
        "for sent in test_sent:\n",
        "\n",
        "  x = np.zeros((1, WORD_LENGTH, len(unique_words)))    \n",
        "\n",
        "  for t, char in enumerate(sent.split()):\n",
        "      if t<WORD_LENGTH:\n",
        "          x[0, t, unique_word_index.get(char,np.random.randint(0,len(unique_words)))] = 1.   \n",
        "\n",
        "  preds = model.predict(x, verbose=0)[0]\n",
        "  next_indices_new = sample(preds, 3)   \n",
        "  top_3_words = unique_words\n",
        "  print(\"Sentence : \",sent)\n",
        "  print(\"Top 3 suggestions :\",unique_words[next_indices_new[0]],\",\",unique_words[next_indices_new[1]],\",\",unique_words[next_indices_new[2]])\n",
        "  next_char_new = unique_words[next_indices_new[0]]\n",
        "  gen = sent+next_char_new\n",
        "  print(gen)     \n",
        "  print(\"=======================================\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4yK-L5_Vdvh",
        "outputId": "06db985a-1f9e-410a-93a1-9c1ae6be5a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence :  i was expecting that \n",
            "Top 3 suggestions : it , her , to\n",
            "i was expecting that it\n",
            "=======================================\n",
            "Sentence :  the one that kills us \n",
            "Top 3 suggestions : alone , as , below\n",
            "the one that kills us alone\n",
            "=======================================\n",
            "Sentence :  could you please step out for \n",
            "Top 3 suggestions : with , of , now\n",
            "could you please step out for with\n",
            "=======================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = \"let me see if you are\"\n",
        "gen = test\n",
        "\n",
        "#print(gen)\n",
        "next_char_new=\"\"\n",
        "newx = np.zeros((1, 5, len(Y[0])))    \n",
        "add_words = 3\n",
        "\n",
        "for i in range(6):\n",
        "  #print(gen)\n",
        "  newx = np.zeros((1, 5, len(Y[0])))    \n",
        "  \n",
        "  for t, char in enumerate(gen.split()[i+1:]):\n",
        "  #for t, char in enumerate(gen.split()[i+add_words:]):\n",
        "        newx[0,t, unique_word_index.get(char)] = 1.   \n",
        "        \n",
        "  preds = model.predict(newx, verbose=0)[0]\n",
        "  next_indices_new = sample(preds, add_words)   \n",
        "  #print(next_indices_new)\n",
        "  #for j in range(add_words):\n",
        "    #next_char_new = unique_words[next_indices_new[j]]\n",
        "  next_char_new = unique_words[np.random.choice(next_indices_new)]\n",
        "  gen = gen+\" \"+next_char_new\n",
        "  print(gen)\n",
        "  #print(next_char_new)\n",
        "  print()  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R76IdKjCJmEy",
        "outputId": "0dd0d606-91e4-4b69-bd1b-df7aa640c652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "let me see if you are not\n",
            "\n",
            "let me see if you are not so\n",
            "\n",
            "let me see if you are not so anxious\n",
            "\n",
            "let me see if you are not so anxious to\n",
            "\n",
            "let me see if you are not so anxious to have\n",
            "\n",
            "let me see if you are not so anxious to have he\n",
            "\n"
          ]
        }
      ]
    }
  ]
}